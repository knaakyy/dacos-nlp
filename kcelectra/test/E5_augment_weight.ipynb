{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## ğŸ§ª E5: ë°ì´í„° ì¦ê°• + ê°€ì¤‘ì¹˜ ë°˜ì˜ ì‹¤í—˜\n",
        "\n",
        "> **ëª©í‘œ**: E4 ê¸°ë°˜ + ì‹¤ì œ ë°ì´í„° ë¹„ìœ¨(ì •ìƒ:ì•…ì„± = 4:1) ë°˜ì˜ + íŠ¹ìˆ˜ë¬¸ì ì •ìƒ ê°•í™”\n",
        "\n",
        "### ë°°ê²½\n",
        "- E4ì—ì„œ LOL ìš•ì„¤ + íŠ¹ìˆ˜ë¬¸ì ì •ìƒ ë°ì´í„° ì¦ê°•ìœ¼ë¡œ ì„±ëŠ¥ í–¥ìƒ\n",
        "- ì‹¤ì œ ìš´ì˜ í™˜ê²½: ì •ìƒ:ì•…ì„± â‰ˆ 4:1\n",
        "- íŠ¹ìˆ˜ë¬¸ì(`@`, `^`, `*` ë“±) ì˜¤íƒ ë¬¸ì œ í•´ê²° í•„ìš”\n",
        "\n",
        "### ì‹¤í—˜ ë‚´ìš©\n",
        "1. **íŠ¹ìˆ˜ë¬¸ì ì •ìƒ ë°ì´í„° ì¶”ê°€ ê°•í™”** â†’ FP ê°ì†Œ\n",
        "2. **class_weight ì ìš©** (w0=4, w1=1) â†’ ì‹¤ì œ ë¹„ìœ¨ ë°˜ì˜\n",
        "\n",
        "### ë³€ê²½ ì‚¬í•­\n",
        "- ë°ì´í„°: E4 + íŠ¹ìˆ˜ë¬¸ì ì •ìƒ ì¶”ê°€\n",
        "- `class_weights`: **[4, 1]** (ì •ìƒ 4ë°° ì¤‘ì‹œ)\n",
        "- `metric_for_best_model`: f1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ========================================\n",
        "# ğŸ“¦ íŒ¨í‚¤ì§€ ì„¤ì¹˜ (í•„ìš”ì‹œ)\n",
        "# ========================================\n",
        "%pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118 -q\n",
        "%pip install transformers==4.42.0 datasets accelerate scikit-learn pandas matplotlib seaborn -q"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ========================================\n",
        "# ğŸ”§ ê²½ë¡œ ì„¤ì •\n",
        "# ========================================\n",
        "import os\n",
        "\n",
        "DATA_DIR = r\"c:\\00_Sookmyunguniv\\DACOS(2025)\\2í•™ê¸°\\í”„ë¡œì íŠ¸\\github\\25-2-team3\\data\"\n",
        "OUTPUT_DIR = r\"./E5_output\"\n",
        "\n",
        "# ë°ì´í„° íŒŒì¼ ê²½ë¡œ\n",
        "MAIN_DATA = os.path.join(DATA_DIR, \"combined_abusive_shuffled_20k.csv\")\n",
        "NORMAL_DATA = os.path.join(DATA_DIR, \"nonabusive_merged_shuffled_sample20000.csv\")\n",
        "LOL_DATA = os.path.join(DATA_DIR, \"LOL_badwords_all_merged.csv\")\n",
        "\n",
        "if not os.path.exists(DATA_DIR):\n",
        "    print(f\"âš ï¸ ë°ì´í„° í´ë”ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {DATA_DIR}\")\n",
        "else:\n",
        "    print(f\"ğŸ“‚ ë°ì´í„° í´ë”: {DATA_DIR}\")\n",
        "print(f\"ğŸ“ ì¶œë ¥ í´ë”: {OUTPUT_DIR}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ========================================\n",
        "# â­ ì‹¤í—˜ íŒŒë¼ë¯¸í„° ì„¤ì •\n",
        "# ========================================\n",
        "\n",
        "# í´ë˜ìŠ¤ ê°€ì¤‘ì¹˜ ì„¤ì • (ì •ìƒ:ì•…ì„± = 4:1 ë°˜ì˜)\n",
        "# ì‹¤ì œ í™˜ê²½ì—ì„œ ì •ìƒì´ 4ë°° ë§ìœ¼ë¯€ë¡œ, ì •ìƒ ì˜¤ë¶„ë¥˜(FP)ì— ë” í° ë²Œì \n",
        "W0, W1 = 4.0, 1.0  # ì •ìƒ 4ë°° ì¤‘ì‹œ â†’ FP ê°ì†Œ\n",
        "\n",
        "print(f\"âš–ï¸ Class weights: [w0={W0}, w1={W1}]\")\n",
        "print(f\"   â†’ ì •ìƒ {W0/W1:.0f}ë°° ì¤‘ì‹œ (FP ê°ì†Œ, ì‹¤ì œ ë¹„ìœ¨ ë°˜ì˜)\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ========================================\n",
        "# ğŸ“š ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„í¬íŠ¸\n",
        "# ========================================\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import random\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import (\n",
        "    accuracy_score, precision_score, recall_score, f1_score,\n",
        "    confusion_matrix, roc_auc_score\n",
        ")\n",
        "from datasets import Dataset, DatasetDict\n",
        "from transformers import (\n",
        "    AutoTokenizer,\n",
        "    AutoModelForSequenceClassification,\n",
        "    TrainingArguments,\n",
        "    Trainer\n",
        ")\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# GPU í™•ì¸\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "if torch.cuda.is_available():\n",
        "    print(f\"âœ… GPU ì‚¬ìš© ê°€ëŠ¥: {torch.cuda.get_device_name(0)}\")\n",
        "else:\n",
        "    print(\"âš ï¸ GPU ì‚¬ìš© ë¶ˆê°€, CPUë¡œ ì‹¤í–‰\")\n",
        "\n",
        "# í•œê¸€ í°íŠ¸ ì„¤ì •\n",
        "plt.rcParams['font.family'] = 'Malgun Gothic'\n",
        "plt.rcParams['axes.unicode_minus'] = False"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ========================================\n",
        "# ğŸ“Š 1) ê¸°ì¡´ ë°ì´í„° ë¡œë“œ\n",
        "# ========================================\n",
        "df_main = pd.read_csv(MAIN_DATA)\n",
        "df_normal = pd.read_csv(NORMAL_DATA)\n",
        "\n",
        "# ê¸°ì¡´ ë°ì´í„° í•©ì¹˜ê¸° (text, label ì»¬ëŸ¼ í†µì¼)\n",
        "if 'text' not in df_main.columns:\n",
        "    df_main = df_main.rename(columns={df_main.columns[0]: 'text'})\n",
        "if 'label' not in df_main.columns:\n",
        "    df_main['label'] = 1  # ì•…ì„±\n",
        "\n",
        "if 'text' not in df_normal.columns:\n",
        "    df_normal = df_normal.rename(columns={df_normal.columns[0]: 'text'})\n",
        "if 'label' not in df_normal.columns:\n",
        "    df_normal['label'] = 0  # ì •ìƒ\n",
        "\n",
        "df_base = pd.concat([df_main[['text', 'label']], df_normal[['text', 'label']]], ignore_index=True)\n",
        "print(f\"ğŸ“Š ê¸°ì¡´ ë°ì´í„°: {len(df_base)}ê°œ\")\n",
        "print(f\"   - ì •ìƒ: {len(df_base[df_base['label']==0])}ê°œ\")\n",
        "print(f\"   - ì•…ì„±: {len(df_base[df_base['label']==1])}ê°œ\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ========================================\n",
        "# ğŸ® 2) LOL ìš•ì„¤ ë°ì´í„° ì¶”ê°€\n",
        "# ========================================\n",
        "df_lol = pd.read_csv(LOL_DATA)\n",
        "print(f\"ğŸ“Š LOL ìš•ì„¤ ì›ë³¸: {len(df_lol)}ê°œ í–‰\")\n",
        "\n",
        "# origin(ì›ë³¸ ìš•ì„¤) + variant(ë³€í˜• ìš•ì„¤) ëª¨ë‘ ì‚¬ìš©\n",
        "lol_origins = df_lol['origin'].dropna().unique().tolist()\n",
        "lol_variants = df_lol['variant'].dropna().unique().tolist()\n",
        "\n",
        "# í•©ì³ì„œ ì¤‘ë³µ ì œê±°\n",
        "all_lol_words = list(set(lol_origins + lol_variants))\n",
        "print(f\"   - origin(ì›ë³¸): {len(lol_origins)}ê°œ\")\n",
        "print(f\"   - variant(ë³€í˜•): {len(lol_variants)}ê°œ\")\n",
        "print(f\"   - í•©ê³„(ì¤‘ë³µì œê±°): {len(all_lol_words)}ê°œ\")\n",
        "\n",
        "df_lol_aug = pd.DataFrame({\n",
        "    'text': all_lol_words,\n",
        "    'label': 1  # ì•…ì„±\n",
        "})\n",
        "\n",
        "# ê¸°ì¡´ ë°ì´í„°ì™€ ì¤‘ë³µ ì œê±°\n",
        "existing_texts = set(df_base['text'].astype(str).str.lower())\n",
        "df_lol_aug = df_lol_aug[~df_lol_aug['text'].astype(str).str.lower().isin(existing_texts)]\n",
        "\n",
        "print(f\"âœ… LOL ìš•ì„¤ ì¶”ê°€: {len(df_lol_aug)}ê°œ (ê¸°ì¡´ ë°ì´í„°ì™€ ì¤‘ë³µ ì œê±° í›„)\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ========================================\n",
        "# ğŸ”£ 3) íŠ¹ìˆ˜ë¬¸ì ì •ìƒ ë°ì´í„° ìƒì„± (ê°•í™” ë²„ì „)\n",
        "# ========================================\n",
        "# E4ë³´ë‹¤ ë” ë§ì€ íŠ¹ìˆ˜ë¬¸ì íŒ¨í„´ ì¶”ê°€\n",
        "special_char_samples = [\n",
        "    # ë‹¨ì¼ íŠ¹ìˆ˜ë¬¸ì\n",
        "    \"@\", \"#\", \"$\", \"%\", \"^\", \"&\", \"*\", \"!\", \"?\", \"~\",\n",
        "    \".\", \",\", \";\", \":\", \"'\", \"\\\"\", \"`\",\n",
        "    \"(\", \")\", \"[\", \"]\", \"{\", \"}\", \"<\", \">\",\n",
        "    \"/\", \"\\\\\", \"|\", \"-\", \"_\", \"=\", \"+\",\n",
        "    \n",
        "    # ì´ëª¨í‹°ì½˜/í‘œì • (ê°•í™”)\n",
        "    \"^^\", \"^0^\", \"^_^\", \"^^;\", \"^o^\", \"^^7\", \"^-^\", \"^ã…^\", \"^ã…¡^\",\n",
        "    \":)\", \":(\", \":D\", \":P\", \";)\", \":/\", \":3\", \":>\", \":<\",\n",
        "    \"ã…‹\", \"ã…\", \"ã… \", \"ã…œ\", \"ã„·\", \"ã„±\",\n",
        "    \"ã…‹ã…‹\", \"ã…ã…\", \"ã… ã… \", \"ã…œã…œ\", \"ã„·ã„·\", \"ã„±ã„±\",\n",
        "    \"ã…‹ã…‹ã…‹\", \"ã…ã…ã…\", \"ã… ã… ã… \", \"ã…œã…œã…œ\",\n",
        "    \"ã…‹ã…‹ã…‹ã…‹ã…‹\", \"ã…ã…ã…ã…ã…\", \"ã… ã… ã… ã… ã… \",\n",
        "    \"ã…‹ã…‹ã…‹ã…‹ã…‹ã…‹ã…‹ã…‹ã…‹ã…‹\", \"ã…ã…ã…ã…ã…ã…ã…ã…ã…ã…\",\n",
        "    \n",
        "    # íŠ¹ìˆ˜ë¬¸ì ì¡°í•© (ê°•í™”)\n",
        "    \"@@@\", \"###\", \"***\", \"!!!\", \"???\", \"~~~\",\n",
        "    \"...\", \"....\", \".....\", \"......\",\n",
        "    \"!?\", \"?!\", \"!?!\", \"?!?\", \"!!??\",\n",
        "    \"ã…‹ã…\", \"ã…ã…‹\", \"ã…‹ã…‹ã…ã…\", \"ã…ã…ã…‹ã…‹\",\n",
        "    \"^^\", \"^^;\", \"^^;;\", \"^^;;;\",\n",
        "    \n",
        "    # ë„ì–´ì“°ê¸° í¬í•¨ (ê°•í™”)\n",
        "    \"^ ^\", \"^ _ ^\", \". . .\", \"? ? ?\", \"! ! !\",\n",
        "    \"ã…‹ ã…‹ ã…‹\", \"ã… ã… ã…\", \"ã…  ã…  ã… \",\n",
        "    \"^ 0 ^\", \"^ - ^\",\n",
        "    \n",
        "    # ë©˜ì…˜/íƒœê·¸ í˜•ì‹ (ê°•í™”)\n",
        "    \"@user\", \"@ë‹˜\", \"#íƒœê·¸\", \"#í•´ì‹œíƒœê·¸\",\n",
        "    \"@all\", \"@everyone\", \"@here\",\n",
        "    \"@admin\", \"@mod\", \"@íŒ€ì¥\", \"@ë‹˜ë“¤\",\n",
        "    \"#ê³µì§€\", \"#ì´ë²¤íŠ¸\", \"#ê¿€íŒ\", \"#ì •ë³´\",\n",
        "    \n",
        "    # ìˆ«ì+íŠ¹ìˆ˜ë¬¸ì (ê°•í™”)\n",
        "    \"1234\", \"12345\", \"123456\", \"1\", \"2\", \"3\", \"0\",\n",
        "    \"100\", \"200\", \"300\", \"1000\", \"10000\",\n",
        "    \"1!\", \"2@\", \"3#\", \"4$\", \"5%\",\n",
        "    \"100%\", \"50%\", \"10%\", \"1ìœ„\", \"2ìœ„\", \"3ìœ„\",\n",
        "    \n",
        "    # ì§§ì€ ì •ìƒ ë¬¸ìì—´ (ê°•í™”)\n",
        "    \"ã…‡ã…‡\", \"ã…‡ã…‹\", \"ã„´ã„´\", \"ã„±ã„±\", \"ã„´ã…‡\", \"ã…‡ã„´\",\n",
        "    \"ã…ã…‡\", \"ã…‚ã…‚\", \"ã„±ã……\", \"ã…Šã…‹\", \"ã„³\",\n",
        "    \"ã…‡ã…‡ã…‡\", \"ã„´ã„´ã„´\", \"ã„±ã„±ã„±\",\n",
        "    \"ok\", \"OK\", \"ã…‡ã…‹\", \"ì˜¤í‚¤\", \"ì˜¤ì¼€ì´\",\n",
        "    \"ã„±ã„±ã„±ã„±\", \"ê³ ê³ ê³ ê³ \", \"ê³ ê³ \",\n",
        "    \"ã…‡ã…‡ã…‡ã…‡\", \"ã„´ã„´ã„´ã„´\",\n",
        "    \n",
        "    # ê²Œì„/ì¸í„°ë„· ì€ì–´ (ì •ìƒ)\n",
        "    \"gg\", \"GG\", \"wp\", \"WP\", \"glhf\", \"GLHF\",\n",
        "    \"lol\", \"LOL\", \"lmao\", \"LMAO\",\n",
        "    \"ã„¹ã…‡\", \"ã…‡ã…ˆ\",\n",
        "    \"ez\", \"EZ\", \"ã…Šã…Š\", \"ã…ã…Œ\",\n",
        "    \n",
        "    # ì¶”ê°€: ë‹¨ìˆœ ë°˜ë³µ íŒ¨í„´\n",
        "    \"ã…‹ã…‹ã…‹ã…‹ã…‹ã…‹\", \"ã…ã…ã…ã…ã…ã…\", \"ã…œã…œã…œã…œã…œã…œ\", \"ã… ã… ã… ã… ã… ã… \",\n",
        "    \"ã„·ã„·ã„·ã„·\", \"ã„±ã„±ã„±ã„±\", \"ã…‡ã…‡ã…‡ã…‡ã…‡\",\n",
        "    \n",
        "    # ì¶”ê°€: í˜¼í•© ì´ëª¨í‹°ì½˜\n",
        "    \"ã…‹ã…‹^^\", \"ã…ã…^^\", \"^^ã…‹ã…‹\", \"ã… ã… ;;\", \"ã…œã…œ;;\",\n",
        "    \"ã…‹ã…‹ã…‹??\", \"ã…ã…ã…!!\", \"??ã…‹ã…‹\", \"!!ã…ã…\",\n",
        "    \n",
        "    # ì¶”ê°€: URL/ì´ë©”ì¼ íŒ¨í„´ (ì •ìƒ)\n",
        "    \"www\", \"http\", \"https\", \".com\", \".kr\", \".net\",\n",
        "    \n",
        "    # ì¶”ê°€: ì‹œê°„/ë‚ ì§œ íŒ¨í„´\n",
        "    \"ì˜¤ì „\", \"ì˜¤í›„\", \"AM\", \"PM\", \"24ì‹œ\", \"365ì¼\",\n",
        "]\n",
        "\n",
        "df_special = pd.DataFrame({\n",
        "    'text': special_char_samples,\n",
        "    'label': 0  # ì •ìƒ\n",
        "})\n",
        "\n",
        "# ì¤‘ë³µ ì œê±°\n",
        "df_special = df_special[~df_special['text'].astype(str).str.lower().isin(existing_texts)]\n",
        "\n",
        "print(f\"âœ… íŠ¹ìˆ˜ë¬¸ì ì •ìƒ ë°ì´í„° ì¶”ê°€: {len(df_special)}ê°œ (E4ë³´ë‹¤ ê°•í™”)\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ========================================\n",
        "# ğŸ“Š 4) ë°ì´í„° í•©ì¹˜ê¸°\n",
        "# ========================================\n",
        "df = pd.concat([df_base, df_lol_aug, df_special], ignore_index=True)\n",
        "\n",
        "# ì…”í”Œ\n",
        "df = df.sample(frac=1, random_state=42).reset_index(drop=True)\n",
        "\n",
        "print(f\"\\nğŸ“Š ìµœì¢… ë°ì´í„° ë¶„í¬:\")\n",
        "print(f\"   ì´ ë°ì´í„°: {len(df)}ê°œ\")\n",
        "print(f\"   - ì •ìƒ(0): {len(df[df['label']==0])}ê°œ\")\n",
        "print(f\"   - ì•…ì„±(1): {len(df[df['label']==1])}ê°œ\")\n",
        "print(f\"   - ì•…ì„±:ì •ìƒ ë¹„ìœ¨: {len(df[df['label']==1])/len(df[df['label']==0]):.2f}:1\")\n",
        "\n",
        "print(f\"\\nğŸ“ˆ ë°ì´í„° ì¦ê°€:\")\n",
        "print(f\"   ê¸°ì¡´: {len(df_base)}ê°œ\")\n",
        "print(f\"   LOL ìš•ì„¤ ì¶”ê°€: +{len(df_lol_aug)}ê°œ\")\n",
        "print(f\"   íŠ¹ìˆ˜ë¬¸ì ì •ìƒ ì¶”ê°€: +{len(df_special)}ê°œ\")\n",
        "print(f\"   ìµœì¢…: {len(df)}ê°œ\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ========================================\n",
        "# ğŸ”€ 5) Train/Test ë¶„í• \n",
        "# ========================================\n",
        "train_df, test_df = train_test_split(\n",
        "    df, test_size=0.2, random_state=42, stratify=df['label']\n",
        ")\n",
        "print(f\"Train: {len(train_df)}, Test: {len(test_df)}\")\n",
        "print(f\"Train ë¹„ìœ¨ - ì •ìƒ: {len(train_df[train_df['label']==0])}, ì•…ì„±: {len(train_df[train_df['label']==1])}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ========================================\n",
        "# ğŸ”¤ 6) í† í°í™”\n",
        "# ========================================\n",
        "MODEL_ID = \"beomi/KcELECTRA-base-v2022\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(MODEL_ID)\n",
        "\n",
        "def tokenize_fn(batch):\n",
        "    return tokenizer(batch[\"text\"], padding=\"max_length\", truncation=True, max_length=128)\n",
        "\n",
        "raw_ds = DatasetDict({\n",
        "    \"train\": Dataset.from_pandas(train_df),\n",
        "    \"validation\": Dataset.from_pandas(test_df),\n",
        "})\n",
        "\n",
        "tokenized = raw_ds.map(tokenize_fn, batched=True, remove_columns=[\"text\"])\n",
        "tokenized = tokenized.rename_column(\"label\", \"labels\")\n",
        "tokenized.set_format(\"torch\")\n",
        "\n",
        "print(\"âœ… í† í°í™” ì™„ë£Œ!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ========================================\n",
        "# âš–ï¸ 7) ì»¤ìŠ¤í…€ ê°€ì¤‘ì¹˜ ì„¤ì •\n",
        "# ========================================\n",
        "class_weights = torch.tensor([W0, W1], dtype=torch.float)\n",
        "print(f\"â­ ì»¤ìŠ¤í…€ Class weights: {class_weights}\")\n",
        "print(f\"   ì •ìƒ(0) ê°€ì¤‘ì¹˜: {W0} â†’ FP ë²Œì  ê°•í™”\")\n",
        "print(f\"   ì•…ì„±(1) ê°€ì¤‘ì¹˜: {W1}\")\n",
        "print(f\"   ì •ìƒ:ì•…ì„± = {W0}:{W1} = {W0/W1:.0f}:1 (ì‹¤ì œ ë°ì´í„° ë¹„ìœ¨ ë°˜ì˜)\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ========================================\n",
        "# ğŸ‹ï¸ 8) ëª¨ë¸ ë° Trainer ì„¤ì •\n",
        "# ========================================\n",
        "def compute_metrics(eval_pred):\n",
        "    logits, labels = eval_pred\n",
        "    preds = np.argmax(logits, axis=1)\n",
        "    \n",
        "    acc = accuracy_score(labels, preds)\n",
        "    prec = precision_score(labels, preds, zero_division=0)\n",
        "    rec = recall_score(labels, preds, zero_division=0)\n",
        "    f1 = f1_score(labels, preds, zero_division=0)\n",
        "    \n",
        "    cm = confusion_matrix(labels, preds)\n",
        "    tn, fp, fn, tp = cm.ravel()\n",
        "    fpr = fp / (fp + tn) if (fp + tn) > 0 else 0\n",
        "    \n",
        "    try:\n",
        "        probs = torch.softmax(torch.tensor(logits), dim=1).numpy()[:, 1]\n",
        "        auc = roc_auc_score(labels, probs)\n",
        "    except:\n",
        "        auc = float(\"nan\")\n",
        "    \n",
        "    return {\n",
        "        \"accuracy\": acc,\n",
        "        \"precision\": prec,\n",
        "        \"recall\": rec,\n",
        "        \"f1\": f1,\n",
        "        \"roc_auc\": auc,\n",
        "        \"FP\": fp,\n",
        "        \"FN\": fn,\n",
        "        \"FPR\": fpr,\n",
        "    }\n",
        "\n",
        "base_model = AutoModelForSequenceClassification.from_pretrained(MODEL_ID, num_labels=2)\n",
        "\n",
        "# â­ í•µì‹¬: ì»¤ìŠ¤í…€ ê°€ì¤‘ì¹˜ ì ìš©\n",
        "class WeightedTrainer(Trainer):\n",
        "    def compute_loss(self, model, inputs, return_outputs=False, num_items_in_batch=None):\n",
        "        labels = inputs.pop(\"labels\")\n",
        "        outputs = model(**inputs)\n",
        "        logits = outputs.logits\n",
        "        \n",
        "        # â­ ì»¤ìŠ¤í…€ ê°€ì¤‘ì¹˜ ì‚¬ìš©\n",
        "        loss_fct = nn.CrossEntropyLoss(weight=class_weights.to(logits.device))\n",
        "        loss = loss_fct(logits, labels)\n",
        "        return (loss, outputs) if return_outputs else loss\n",
        "\n",
        "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
        "\n",
        "args = TrainingArguments(\n",
        "    output_dir=OUTPUT_DIR,\n",
        "    eval_strategy=\"epoch\",\n",
        "    save_strategy=\"epoch\",\n",
        "    load_best_model_at_end=True,\n",
        "    metric_for_best_model=\"f1\",\n",
        "    learning_rate=3e-5,\n",
        "    per_device_train_batch_size=32,\n",
        "    per_device_eval_batch_size=32,\n",
        "    num_train_epochs=3,\n",
        "    warmup_ratio=0.1,\n",
        "    weight_decay=0.01,\n",
        "    logging_steps=50,\n",
        "    fp16=torch.cuda.is_available(),\n",
        "    report_to=\"none\"\n",
        ")\n",
        "\n",
        "trainer = WeightedTrainer(\n",
        "    model=base_model,\n",
        "    args=args,\n",
        "    train_dataset=tokenized[\"train\"],\n",
        "    eval_dataset=tokenized[\"validation\"],\n",
        "    tokenizer=tokenizer,\n",
        "    compute_metrics=compute_metrics,\n",
        ")\n",
        "\n",
        "print(\"âœ… Trainer ì„¤ì • ì™„ë£Œ!\")\n",
        "print(f\"ğŸ“Œ E5 íŠ¹ì§•: E4 ë°ì´í„° + class_weights=[{W0}, {W1}] (ì •ìƒ 4ë°°)\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ========================================\n",
        "# ğŸš€ 9) í•™ìŠµ ì‹¤í–‰\n",
        "# ========================================\n",
        "trainer.train()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ========================================\n",
        "# ğŸ“Š 10) ìµœì¢… í‰ê°€\n",
        "# ========================================\n",
        "final_metrics = trainer.evaluate()\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(f\"ğŸ“Š E5 ìµœì¢… í‰ê°€ ê²°ê³¼ (ë°ì´í„° ì¦ê°• + ê°€ì¤‘ì¹˜ {W0}:{W1})\")\n",
        "print(\"=\"*60)\n",
        "for key, value in final_metrics.items():\n",
        "    if isinstance(value, float):\n",
        "        print(f\"  {key}: {value:.4f}\")\n",
        "    else:\n",
        "        print(f\"  {key}: {value}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ========================================\n",
        "# ğŸ“ 11) ì‹¤í—˜ ê²°ê³¼ ê¸°ë¡\n",
        "# ========================================\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"ğŸ“ ì‹¤í—˜ ê²°ê³¼ ê¸°ë¡ (ë³µì‚¬ìš©)\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "# ë³€ìˆ˜ ë¯¸ë¦¬ í¬ë§·íŒ…\n",
        "fpr_str = f\"{final_metrics['eval_FPR']:.4f}\" if final_metrics.get('eval_FPR') is not None else 'N/A'\n",
        "prec_str = f\"{final_metrics['eval_precision']:.4f}\" if final_metrics.get('eval_precision') is not None else 'N/A'\n",
        "rec_str = f\"{final_metrics['eval_recall']:.4f}\" if final_metrics.get('eval_recall') is not None else 'N/A'\n",
        "f1_str = f\"{final_metrics['eval_f1']:.4f}\" if final_metrics.get('eval_f1') is not None else 'N/A'\n",
        "auc_str = f\"{final_metrics['eval_roc_auc']:.4f}\" if final_metrics.get('eval_roc_auc') is not None else 'N/A'\n",
        "\n",
        "print(f\"\"\"\n",
        "#### ì‹¤í—˜ ID: E5\n",
        "- ë³€ê²½ ì‚¬í•­:\n",
        "  - ë°ì´í„° ì¦ê°•: LOL ìš•ì„¤({len(df_lol_aug)}ê°œ) + íŠ¹ìˆ˜ë¬¸ì ì •ìƒ({len(df_special)}ê°œ, ê°•í™”)\n",
        "  - ì´ ë°ì´í„°: {len(df)}ê°œ (ê¸°ì¡´ {len(df_base)}ê°œ â†’ +{len(df)-len(df_base)}ê°œ)\n",
        "  - class_weights: [{W0}, {W1}] (ì •ìƒ {int(W0/W1)}ë°° ì¤‘ì‹œ)\n",
        "  - metric_for_best_model: f1\n",
        "- ë°ì´í„°:\n",
        "  - train/validation split: 0.8/0.2 (random_state=42, stratify=label)\n",
        "  - max_length=128, batch_size=32, epochs=3\n",
        "- Validation ì„±ëŠ¥:\n",
        "  - FP(ì •ìƒâ†’ì•…ì„±): {final_metrics.get('eval_FP', 'N/A')}\n",
        "  - FN(ì•…ì„±â†’ì •ìƒ): {final_metrics.get('eval_FN', 'N/A')}\n",
        "  - FPR: {fpr_str}\n",
        "  - Precision: {prec_str}\n",
        "  - Recall: {rec_str}\n",
        "  - F1: {f1_str}\n",
        "  - AUC: {auc_str}\n",
        "- ì½”ë©˜íŠ¸:\n",
        "  - E4 ëŒ€ë¹„ FP â†“ ì˜ˆìƒ (ì •ìƒ ê°€ì¤‘ì¹˜ ê°•í™”)\n",
        "  - ì‹¤ì œ ë°ì´í„° ë¹„ìœ¨(4:1) ë°˜ì˜\n",
        "\"\"\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ========================================\n",
        "# ğŸ’¾ 12) ëª¨ë¸ ì €ì¥\n",
        "# ========================================\n",
        "SAVE_DIR = os.path.join(OUTPUT_DIR, \"best_model\")\n",
        "os.makedirs(SAVE_DIR, exist_ok=True)\n",
        "trainer.save_model(SAVE_DIR)\n",
        "tokenizer.save_pretrained(SAVE_DIR)\n",
        "\n",
        "print(f\"âœ… ëª¨ë¸ ì €ì¥ ì™„ë£Œ: {SAVE_DIR}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "## ğŸ“‹ E4 vs E5 ë¹„êµ\n",
        "\n",
        "| í•­ëª© | E4 | E5 |\n",
        "|------|-----|-----|\n",
        "| ë°ì´í„° ì¦ê°• | LOL ìš•ì„¤ + íŠ¹ìˆ˜ë¬¸ì ì •ìƒ | ë™ì¼ (íŠ¹ìˆ˜ë¬¸ì ê°•í™”) |\n",
        "| class_weights | ê¸°ë³¸ (1:1) | **[4, 1]** (ì •ìƒ 4ë°°) |\n",
        "| ì˜ˆìƒ íš¨ê³¼ | ì•…ì„± íƒì§€â†‘ | **FP â†“** (ì •ìƒ ì˜¤ë¶„ë¥˜ ê°ì†Œ) |\n",
        "\n",
        "---\n",
        "## ğŸ“‹ ë‹¤ìŒ ë‹¨ê³„\n",
        "\n",
        "E5 ê²°ê³¼ í™•ì¸ í›„:\n",
        "1. **Threshold íŠœë‹**: 0.7 ê¸°ì¤€ìœ¼ë¡œ ì¡°ì •\n",
        "2. **ëª¨ë¸ë¹„êµ_í…ŒìŠ¤íŠ¸.ipynb**: E4, E5 ë¹„êµ í‰ê°€"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "kcelectra",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.16"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
