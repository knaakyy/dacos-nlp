{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## ğŸ§ª E3: Loss ê°€ì¤‘ì¹˜ ì¡°ì • ì‹¤í—˜\n",
        "\n",
        "> **ëª©í‘œ**: class weight ì¡°ì •ìœ¼ë¡œ **ì‹¤ì œ ë°ì´í„° ë¹„ìœ¨ ë°˜ì˜** ë° **FP/FN ê· í˜• ìµœì í™”**\n",
        "\n",
        "### ë°°ê²½\n",
        "- ì‹¤ì œ ìš´ì˜ í™˜ê²½ì—ì„œ **ìš•ì„¤:ì •ìƒ â‰ˆ 1:4** ë¹„ìœ¨\n",
        "- ì•…ì„±(í¬ì†Œ í´ë˜ìŠ¤)ì— ë” ë†’ì€ ê°€ì¤‘ì¹˜ë¥¼ ì¤˜ì•¼ ê· í˜• ì¡íŒ í•™ìŠµ ê°€ëŠ¥\n",
        "- `metric_for_best_model = f1` â†’ Precision/Recall ê· í˜• ìµœì í™” (FP/FN ë™ì‹œ ê°ì†Œ)\n",
        "\n",
        "### ì‹¤í—˜ ë‚´ìš©\n",
        "- `CrossEntropyLoss`ì˜ class weight ì¡°ì •\n",
        "- **w0(ì •ìƒ) â†‘**: ì •ìƒâ†’ì•…ì„± ì˜¤ë¶„ë¥˜ ë²Œì  ì¦ê°€ â†’ FP ê°ì†Œ\n",
        "- **w1(ì•…ì„±) â†‘**: ì•…ì„±â†’ì •ìƒ ì˜¤ë¶„ë¥˜ ë²Œì  ì¦ê°€ â†’ FN ê°ì†Œ\n",
        "\n",
        "### ë³€ê²½ ì‚¬í•­\n",
        "- `metric_for_best_model`: f1 (FP/FN ê· í˜•)\n",
        "- `class_weights`: **[w0, w1] ì¡°ì •** â­\n",
        "- `threshold`: 0.5 (ê¸°ì¡´ ìœ ì§€)\n",
        "\n",
        "### ì‹¤í—˜ ë²„ì „\n",
        "| ë²„ì „ | w0 | w1 | ì„¤ëª… |\n",
        "|------|-----|-----|------|\n",
        "| **E3-R** (ê¶Œì¥) | 1.0 | 4.0 | ì‹¤ì œ ë¹„ìœ¨ ë°˜ì˜ (ì•…ì„± 4ë°°) |\n",
        "| E3-R2 | 1.0 | 2.0 | ì‹¤ì œ ë¹„ìœ¨ ì•½í•˜ê²Œ (ì•…ì„± 2ë°°) |\n",
        "| E3-1 | 1.5 | 0.5 | FP ê°ì†Œ ì¤‘ì‹¬ |\n",
        "| E3-2 | 2.0 | 0.5 | FP ê°•í•˜ê²Œ ê°ì†Œ |\n",
        "| E3-3 | 0.5 | 1.5 | FN ê°ì†Œ ì¤‘ì‹¬ |\n",
        "| E3-4 | 0.25 | 1.0 | FN ê°•í•˜ê²Œ ê°ì†Œ |"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ========================================\n",
        "# ğŸ“¦ íŒ¨í‚¤ì§€ ì„¤ì¹˜ (í•„ìš”ì‹œ)\n",
        "# ========================================\n",
        "%pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118 -q\n",
        "%pip install transformers==4.42.0 datasets accelerate scikit-learn pandas matplotlib seaborn -q"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ========================================\n",
        "# ğŸ”§ ê²½ë¡œ ì„¤ì • (ë³¸ì¸ í™˜ê²½ì— ë§ê²Œ ìˆ˜ì •)\n",
        "# ========================================\n",
        "import os\n",
        "\n",
        "# ì ˆëŒ€ ê²½ë¡œ ì‚¬ìš© (ë³¸ì¸ í™˜ê²½ì— ë§ê²Œ ìˆ˜ì •í•˜ì„¸ìš”)\n",
        "DATA_DIR = r\"c:\\00_Sookmyunguniv\\DACOS(2025)\\2í•™ê¸°\\í”„ë¡œì íŠ¸\\github\\25-2-team3\\data\"\n",
        "OUTPUT_DIR = r\"./E3_output\"\n",
        "\n",
        "# ê²½ë¡œ í™•ì¸\n",
        "if not os.path.exists(DATA_DIR):\n",
        "    print(f\"âš ï¸ ë°ì´í„° í´ë”ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {DATA_DIR}\")\n",
        "    print(f\"   í˜„ì¬ ì‘ì—… ë””ë ‰í† ë¦¬: {os.getcwd()}\")\n",
        "else:\n",
        "    print(f\"ğŸ“‚ ë°ì´í„° í´ë”: {DATA_DIR}\")\n",
        "print(f\"ğŸ“ ì¶œë ¥ í´ë”: {OUTPUT_DIR}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ========================================\n",
        "# â­ ì‹¤í—˜ íŒŒë¼ë¯¸í„° ì„¤ì • (ì—¬ê¸°ì„œ ì¡°ì •!)\n",
        "# ========================================\n",
        "\n",
        "# í´ë˜ìŠ¤ ê°€ì¤‘ì¹˜ ì„¤ì • (w0=ì •ìƒ, w1=ì•…ì„±)\n",
        "# - w0 ë†’ì´ë©´ â†’ FP(ì •ìƒâ†’ì•…ì„± ì˜¤íƒ) ë²Œì  ì¦ê°€\n",
        "# - w1 ë†’ì´ë©´ â†’ FN(ì•…ì„±â†’ì •ìƒ ë¯¸íƒ) ë²Œì  ì¦ê°€\n",
        "\n",
        "# ===== ì‹¤í—˜ ë²„ì „ ì„ íƒ (í•˜ë‚˜ë§Œ ì£¼ì„ í•´ì œ) =====\n",
        "\n",
        "# --- ğŸŒŸ ì‹¤ì œ ë°ì´í„° ë¹„ìœ¨ ë°˜ì˜ (ê¶Œì¥) ---\n",
        "# ì‹¤ì œ ìš´ì˜ í™˜ê²½: ìš•ì„¤:ì •ìƒ â‰ˆ 1:4\n",
        "# â†’ ì•…ì„±(í¬ì†Œ)ì— 4ë°° ê°€ì¤‘ì¹˜ = FP/FN ê· í˜• ìµœì í™”\n",
        "\n",
        "# E3-R: ì‹¤ì œ ë¹„ìœ¨ ë°˜ì˜ (ì•…ì„± 4ë°°)\n",
        "W0, W1 = 1.0, 4.0\n",
        "EXPERIMENT_ID = \"E3-R\"\n",
        "\n",
        "# E3-R2: ì‹¤ì œ ë¹„ìœ¨ ì•½í•˜ê²Œ (ì•…ì„± 2ë°°)\n",
        "# W0, W1 = 1.0, 2.0\n",
        "# EXPERIMENT_ID = \"E3-R2\"\n",
        "\n",
        "# --- FP(ì˜¤íƒ) ê°ì†Œ ì¤‘ì‹¬ ---\n",
        "# E3-1: FP ê°ì†Œ (ì •ìƒ 3ë°° ì¤‘ì‹œ)\n",
        "# W0, W1 = 1.5, 0.5\n",
        "# EXPERIMENT_ID = \"E3-1\"\n",
        "\n",
        "# E3-2: FP ê°•í•˜ê²Œ ê°ì†Œ (ì •ìƒ 4ë°° ì¤‘ì‹œ)\n",
        "# W0, W1 = 2.0, 0.5\n",
        "# EXPERIMENT_ID = \"E3-2\"\n",
        "\n",
        "# --- FN(ë¯¸íƒ) ê°ì†Œ ì¤‘ì‹¬ ---\n",
        "# E3-3: FN ê°ì†Œ (ì•…ì„± 3ë°° ì¤‘ì‹œ)\n",
        "# W0, W1 = 0.5, 1.5\n",
        "# EXPERIMENT_ID = \"E3-3\"\n",
        "\n",
        "# E3-4: FN ê°•í•˜ê²Œ ê°ì†Œ (ì•…ì„± 4ë°° ì¤‘ì‹œ) \n",
        "# W0, W1 = 0.25, 1.0\n",
        "# EXPERIMENT_ID = \"E3-4\"\n",
        "\n",
        "# =============================================\n",
        "\n",
        "print(f\"ğŸ”¬ ì‹¤í—˜ ID: {EXPERIMENT_ID}\")\n",
        "print(f\"âš–ï¸ Class weights: [w0={W0}, w1={W1}]\")\n",
        "ratio = W1 / W0 if W0 > 0 else float('inf')\n",
        "if ratio > 1:\n",
        "    print(f\"   â†’ ì•…ì„± {ratio:.1f}ë°° ì¤‘ì‹œ (FN ê°ì†Œ, ì‹¤ì œ ë¹„ìœ¨ ë°˜ì˜)\")\n",
        "elif ratio < 1:\n",
        "    print(f\"   â†’ ì •ìƒ {1/ratio:.1f}ë°° ì¤‘ì‹œ (FP ê°ì†Œ)\")\n",
        "else:\n",
        "    print(f\"   â†’ ê· í˜• (1:1)\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ========================================\n",
        "# 1) í™˜ê²½ ì¤€ë¹„\n",
        "# ========================================\n",
        "import os\n",
        "import random\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import (precision_recall_fscore_support, accuracy_score, \n",
        "                             roc_auc_score, confusion_matrix)\n",
        "from datasets import Dataset, DatasetDict\n",
        "from transformers import (AutoTokenizer, AutoModelForSequenceClassification, \n",
        "                          Trainer, TrainingArguments)\n",
        "\n",
        "# ì‹œë“œ ê³ ì •\n",
        "random.seed(42)\n",
        "np.random.seed(42)\n",
        "torch.manual_seed(42)\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "    torch.cuda.manual_seed_all(42)\n",
        "    print(f\"âœ… GPU ì‚¬ìš© ê°€ëŠ¥: {torch.cuda.get_device_name(0)}\")\n",
        "else:\n",
        "    print(\"âš ï¸ GPUê°€ ê°ì§€ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. CPUë¡œ ì‹¤í–‰ë©ë‹ˆë‹¤.\")\n",
        "\n",
        "plt.rcParams['font.family'] = 'Malgun Gothic'\n",
        "plt.rcParams['axes.unicode_minus'] = False"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ========================================\n",
        "# 2) ë°ì´í„° ë¡œë“œ\n",
        "# ========================================\n",
        "abusive_path = os.path.join(DATA_DIR, \"combined_abusive_shuffled_20k.csv\")\n",
        "nonabusive_path = os.path.join(DATA_DIR, \"nonabusive_merged_shuffled_sample20000.csv\")\n",
        "\n",
        "df_ab = pd.read_csv(abusive_path)[[\"text\", \"label\"]]\n",
        "df_non = pd.read_csv(nonabusive_path)[[\"text\", \"label\"]]\n",
        "\n",
        "df = pd.concat([df_ab, df_non], ignore_index=True).dropna().reset_index(drop=True)\n",
        "\n",
        "print(f\"ğŸ“Š ë¼ë²¨ ë¶„í¬:\\n{df['label'].value_counts()}\")\n",
        "print(f\"\\nì´ ë°ì´í„° ìˆ˜: {len(df)}ê°œ\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ========================================\n",
        "# 3) Train/Test ë¶„ë¦¬\n",
        "# ========================================\n",
        "train_df, test_df = train_test_split(\n",
        "    df, test_size=0.2, random_state=42, stratify=df[\"label\"]\n",
        ")\n",
        "print(f\"Train: {len(train_df)}, Test: {len(test_df)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ========================================\n",
        "# 4) í† í°í™”\n",
        "# ========================================\n",
        "MODEL_ID = \"beomi/KcELECTRA-base-v2022\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(MODEL_ID)\n",
        "\n",
        "def tokenize_fn(batch):\n",
        "    return tokenizer(batch[\"text\"], padding=\"max_length\", truncation=True, max_length=128)\n",
        "\n",
        "raw_ds = DatasetDict({\n",
        "    \"train\": Dataset.from_pandas(train_df),\n",
        "    \"validation\": Dataset.from_pandas(test_df),\n",
        "})\n",
        "\n",
        "tokenized = raw_ds.map(tokenize_fn, batched=True, remove_columns=[\"text\"])\n",
        "tokenized = tokenized.rename_column(\"label\", \"labels\")\n",
        "tokenized.set_format(\"torch\")\n",
        "\n",
        "print(\"âœ… í† í°í™” ì™„ë£Œ!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ========================================\n",
        "# 5) â­ ì»¤ìŠ¤í…€ í´ë˜ìŠ¤ ê°€ì¤‘ì¹˜ ì„¤ì •\n",
        "# ========================================\n",
        "# ìœ„ì—ì„œ ì„¤ì •í•œ W0, W1 ì‚¬ìš©\n",
        "class_weights = torch.tensor([W0, W1], dtype=torch.float)\n",
        "\n",
        "print(f\"â­ ì»¤ìŠ¤í…€ Class weights: {class_weights}\")\n",
        "print(f\"   ì •ìƒ(0) ê°€ì¤‘ì¹˜: {W0} â†’ FP ë²Œì  ê°•í™”\")\n",
        "print(f\"   ì•…ì„±(1) ê°€ì¤‘ì¹˜: {W1}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ========================================\n",
        "# 6) ëª¨ë¸ + Trainer ì„¤ì •\n",
        "# ========================================\n",
        "def compute_metrics(eval_pred):\n",
        "    logits, labels = eval_pred\n",
        "    preds = np.argmax(logits, axis=1)\n",
        "    acc = accuracy_score(labels, preds)\n",
        "    prec, rec, f1, _ = precision_recall_fscore_support(labels, preds, average=\"binary\", zero_division=0)\n",
        "    \n",
        "    # Confusion matrix for FP\n",
        "    tn, fp, fn, tp = confusion_matrix(labels, preds).ravel()\n",
        "    fpr = fp / (fp + tn) if (fp + tn) > 0 else 0\n",
        "    \n",
        "    try:\n",
        "        probs = torch.softmax(torch.tensor(logits), dim=1).numpy()[:, 1]\n",
        "        auc = roc_auc_score(labels, probs)\n",
        "    except:\n",
        "        auc = float(\"nan\")\n",
        "    \n",
        "    return {\n",
        "        \"accuracy\": acc, \n",
        "        \"precision\": prec, \n",
        "        \"recall\": rec, \n",
        "        \"f1\": f1, \n",
        "        \"roc_auc\": auc,\n",
        "        \"FP\": fp,\n",
        "        \"FN\": fn,\n",
        "        \"FPR\": fpr,\n",
        "    }\n",
        "\n",
        "base_model = AutoModelForSequenceClassification.from_pretrained(MODEL_ID, num_labels=2)\n",
        "\n",
        "# â­ í•µì‹¬: ì»¤ìŠ¤í…€ ê°€ì¤‘ì¹˜ ì ìš©\n",
        "class WeightedTrainer(Trainer):\n",
        "    def compute_loss(self, model, inputs, return_outputs=False, num_items_in_batch=None):\n",
        "        labels = inputs.pop(\"labels\")\n",
        "        outputs = model(**inputs)\n",
        "        logits = outputs.logits\n",
        "        \n",
        "        # â­ ì»¤ìŠ¤í…€ ê°€ì¤‘ì¹˜ ì‚¬ìš©\n",
        "        loss_fct = nn.CrossEntropyLoss(weight=class_weights.to(logits.device))\n",
        "        loss = loss_fct(logits, labels)\n",
        "        return (loss, outputs) if return_outputs else loss\n",
        "\n",
        "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
        "\n",
        "args = TrainingArguments(\n",
        "    output_dir=OUTPUT_DIR,\n",
        "    eval_strategy=\"epoch\",\n",
        "    save_strategy=\"epoch\",\n",
        "    load_best_model_at_end=True,\n",
        "    metric_for_best_model=\"f1\",  # ê¸°ì¡´ ìœ ì§€\n",
        "    learning_rate=3e-5,\n",
        "    per_device_train_batch_size=32,\n",
        "    per_device_eval_batch_size=32,\n",
        "    num_train_epochs=3,\n",
        "    warmup_ratio=0.1,\n",
        "    weight_decay=0.01,\n",
        "    logging_steps=50,\n",
        "    fp16=torch.cuda.is_available(),\n",
        "    report_to=\"none\"\n",
        ")\n",
        "\n",
        "trainer = WeightedTrainer(\n",
        "    model=base_model,\n",
        "    args=args,\n",
        "    train_dataset=tokenized[\"train\"],\n",
        "    eval_dataset=tokenized[\"validation\"],\n",
        "    tokenizer=tokenizer,\n",
        "    compute_metrics=compute_metrics,\n",
        ")\n",
        "\n",
        "print(\"âœ… Trainer ì„¤ì • ì™„ë£Œ!\")\n",
        "print(f\"â­ class_weights = [{W0}, {W1}]\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ========================================\n",
        "# 7) í•™ìŠµ ì‹¤í–‰\n",
        "# ========================================\n",
        "trainer.train()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ========================================\n",
        "# 8) ìµœì¢… í‰ê°€\n",
        "# ========================================\n",
        "final_metrics = trainer.evaluate()\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(f\"ğŸ“Š {EXPERIMENT_ID} ìµœì¢… í‰ê°€ ê²°ê³¼ (weight=[{W0},{W1}])\")\n",
        "print(\"=\"*60)\n",
        "for key, value in final_metrics.items():\n",
        "    if isinstance(value, float):\n",
        "        print(f\"  {key}: {value:.4f}\")\n",
        "    else:\n",
        "        print(f\"  {key}: {value}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ========================================\n",
        "# ğŸ“ 9) ì‹¤í—˜ ê²°ê³¼ ê¸°ë¡\n",
        "# ========================================\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"ğŸ“ ì‹¤í—˜ ê²°ê³¼ ê¸°ë¡ (ë³µì‚¬ìš©)\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "# ë³€ìˆ˜ ë¯¸ë¦¬ í¬ë§·íŒ…\n",
        "fpr_str = f\"{final_metrics['eval_FPR']:.4f}\" if final_metrics.get('eval_FPR') is not None else 'N/A'\n",
        "prec_str = f\"{final_metrics['eval_precision']:.4f}\" if final_metrics.get('eval_precision') is not None else 'N/A'\n",
        "rec_str = f\"{final_metrics['eval_recall']:.4f}\" if final_metrics.get('eval_recall') is not None else 'N/A'\n",
        "f1_str = f\"{final_metrics['eval_f1']:.4f}\" if final_metrics.get('eval_f1') is not None else 'N/A'\n",
        "auc_str = f\"{final_metrics['eval_roc_auc']:.4f}\" if final_metrics.get('eval_roc_auc') is not None else 'N/A'\n",
        "\n",
        "# ì½”ë©˜íŠ¸ ë™ì  ìƒì„±\n",
        "if W0 > W1:\n",
        "    comment = f\"ì •ìƒ ê°€ì¤‘ì¹˜ {W0}ë¡œ FP(ì˜¤íƒ) ë²Œì  ê°•í™”\\n  - ê¸°ì¡´ ëŒ€ë¹„ FP â†“, FN â†‘ ì˜ˆìƒ\"\n",
        "elif W1 > W0:\n",
        "    comment = f\"ì•…ì„± ê°€ì¤‘ì¹˜ {W1}ë¡œ FN(ë¯¸íƒ) ë²Œì  ê°•í™”\\n  - ê¸°ì¡´ ëŒ€ë¹„ FN â†“, FP â†‘ ì˜ˆìƒ\"\n",
        "else:\n",
        "    comment = \"ê· í˜• ê°€ì¤‘ì¹˜\"\n",
        "\n",
        "print(f\"\"\"\n",
        "#### ì‹¤í—˜ ID: {EXPERIMENT_ID}\n",
        "- ë³€ê²½ ì‚¬í•­:\n",
        "  - class weight: [{W0}, {W1}]\n",
        "  - metric_for_best_model: f1 (ê¸°ì¡´ ìœ ì§€)\n",
        "  - threshold: 0.5 (ê¸°ì¡´ ìœ ì§€)\n",
        "- ë°ì´í„°:\n",
        "  - train/validation split: 0.8/0.2 (random_state=42, stratify=label)\n",
        "  - max_length=128, batch_size=32, epochs=3\n",
        "- Validation ì„±ëŠ¥:\n",
        "  - FP(ì •ìƒâ†’ì•…ì„±): {final_metrics.get('eval_FP', 'N/A')}\n",
        "  - FN(ì•…ì„±â†’ì •ìƒ): {final_metrics.get('eval_FN', 'N/A')}\n",
        "  - FPR: {fpr_str}\n",
        "  - Precision: {prec_str}\n",
        "  - Recall: {rec_str}\n",
        "  - F1: {f1_str}\n",
        "  - AUC: {auc_str}\n",
        "- ì½”ë©˜íŠ¸:\n",
        "  - {comment}\n",
        "\"\"\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ========================================\n",
        "# ğŸ’¾ 10) ëª¨ë¸ ì €ì¥\n",
        "# ========================================\n",
        "SAVE_DIR = os.path.join(OUTPUT_DIR, f\"best_model_{EXPERIMENT_ID}\")\n",
        "os.makedirs(SAVE_DIR, exist_ok=True)\n",
        "trainer.save_model(SAVE_DIR)\n",
        "tokenizer.save_pretrained(SAVE_DIR)\n",
        "\n",
        "print(f\"âœ… ëª¨ë¸ ì €ì¥ ì™„ë£Œ: {SAVE_DIR}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "## ğŸ“‹ ì‹¤í—˜ ë²„ì „ ë¹„êµ ê°€ì´ë“œ\n",
        "\n",
        "| ë²„ì „ | w0 (ì •ìƒ) | w1 (ì•…ì„±) | ì •ìƒ:ì•…ì„± ë¹„ìœ¨ | ì˜ˆìƒ íš¨ê³¼ |\n",
        "|------|----------|----------|--------------|----------|\n",
        "| ê¸°ì¡´ | ~0.86 | ~1.14 | 0.75:1 | ì•…ì„± ì¤‘ì‹œ |\n",
        "| E3-1 | 1.5 | 0.5 | 3:1 | FP â†“â†“, FN â†‘ |\n",
        "| E3-2 | 2.0 | 0.5 | 4:1 | FP â†“â†“â†“, FN â†‘â†‘ |\n",
        "| E3-3 | 1.2 | 0.8 | 1.5:1 | ì•½í•œ ì¡°ì • |\n",
        "\n",
        "### ê¶Œì¥ ìˆœì„œ\n",
        "1. E3-3 ë¨¼ì € ì‹¤í–‰ (ì•½í•œ ì¡°ì •)\n",
        "2. FP ê°ì†Œ íš¨ê³¼ í™•ì¸ í›„ E3-1 ì‹œë„\n",
        "3. í•„ìš”ì‹œ E3-2 (ê°•í•œ ì¡°ì •)\n",
        "\n",
        "### ì£¼ì˜ì‚¬í•­\n",
        "- w0ë¥¼ ë„ˆë¬´ ë†’ì´ë©´ ì•…ì„± ëŒ“ê¸€ì„ ë†“ì¹  ìˆ˜ ìˆìŒ (FN ì¦ê°€)\n",
        "- ìš´ì˜ ëª©ì ì— ë§ê²Œ FP vs FN íŠ¸ë ˆì´ë“œì˜¤í”„ ê³ ë ¤"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python (kcelectra)",
      "language": "python",
      "name": "kcelectra"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.25"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
